# Symplectic Gaussian Process Flows | 2022
# Yusuke Tanaka

import sys
import pdb
import torch
import torch.nn as nn
import numpy as np
import math
from sqrtm import sqrtm
import time
torch.set_default_dtype(torch.float64)

from utils import choose_nonlinearity


class MLP(torch.nn.Module):
  def __init__(self, input_dim, hidden_dim, output_dim, nonlinearity='tanh'):
    super(MLP, self).__init__()
    self.linear1 = torch.nn.Linear(input_dim, hidden_dim)
    self.linear2 = torch.nn.Linear(hidden_dim, hidden_dim)
    self.linear3 = torch.nn.Linear(hidden_dim, output_dim, bias=None)

    for l in [self.linear1, self.linear2, self.linear3]:
      torch.nn.init.orthogonal_(l.weight)
      
    self.nonlinearity = choose_nonlinearity(nonlinearity)

  def forward(self, x):
    h = self.nonlinearity( self.linear1(x) )
    h = self.nonlinearity( self.linear2(h) )
    return self.linear3(h)

  
class NN(torch.nn.Module):
  def __init__(self, input_dim, sigma, differentiable_model):
    super(NN, self).__init__()
    self.differentiable_model = differentiable_model
    self.sigma = nn.Parameter(torch.tensor([sigma]))

  def forward(self, t, x):
    return self.differentiable_model(x)

  def neg_loglike(self, xs, dys):
    n_samples, input_dim = dys.shape
    pred_x = self.forward([0], xs)
    likelihood = ( -((pred_x-dys)**2).sum()/self.sigma**2/2
                   - torch.log(self.sigma**2)/2*n_samples*input_dim)
    return -likelihood

  
class HNN(torch.nn.Module):
  def __init__(self, input_dim, sigma, differentiable_model):
    super(HNN, self).__init__()
    self.differentiable_model = differentiable_model
    self.sigma = nn.Parameter(torch.tensor([sigma]))
    self.M = self.permutation_tensor(input_dim)

  def forward(self, t, x):
    x = x.squeeze().clone().detach().requires_grad_(True)
    y = self.differentiable_model(x)
    H, dammy = y.split(1) if y.dim() == 1 else y.split(1,1)
    dH = torch.autograd.grad(H.sum(), x, create_graph=True)[0]
    field = dH @ self.M.t()
    return field

  def neg_loglike(self, ys, dys):
    n_samples, input_dim = dys.shape
    pred_x = self.forward([0], ys)
    likelihood = ( -((pred_x-dys)**2).sum()/self.sigma**2/2
                   - torch.log(self.sigma**2)/2*n_samples*input_dim )
    return -likelihood

  def permutation_tensor(self,n):
    M = torch.eye(n)
    M = torch.cat([M[n//2:], -M[:n//2]])
    return M


class D_HNN(torch.nn.Module):
  def __init__(self, input_dim, sigma, eta, differentiable_model):
    super(D_HNN, self).__init__()
    self.differentiable_model = differentiable_model
    self.sigma = nn.Parameter(torch.tensor([sigma]))
    self.eta = nn.Parameter(torch.tensor([eta]))
    self.M = self.permutation_tensor(input_dim)

  def forward(self, t, x):
    x = x.squeeze().clone().detach().requires_grad_(True)
    y = self.differentiable_model(x)
    H, dammy = y.split(1) if y.dim() == 1 else y.split(1,1)
    dH = torch.autograd.grad(H.sum(), x, create_graph=True)[0]
    field = dH @ self.M.t()
    if len(x) == 2:
      dH[0] = 0
    else:
      dH[:,0] = 0
    field = field - self.eta**2 * dH
    return field

  def neg_loglike(self, ys, dys):
    n_samples, input_dim = dys.shape
    pred_x = self.forward([0], ys)
    likelihood = ( -((pred_x-dys)**2).sum()/self.sigma**2/2
                   - torch.log(self.sigma**2)/2*n_samples*input_dim )
    return -likelihood# + 1000 * self.eta**2

  def permutation_tensor(self,n):
    M = torch.eye(n)
    M = torch.cat([M[n//2:], -M[:n//2]])
    return M

  
class NODE(torch.nn.Module):
  def __init__(self, input_dim, sigma, differentiable_model):
    super(NODE, self).__init__()
    self.differentiable_model = differentiable_model
    self.sigma = nn.Parameter(torch.tensor([sigma]))
    #self.a = nn.Parameter(torch.tensor([1e-1]))
    self.a = nn.Parameter(torch.ones(input_dim)*1e-1)

  def forward(self, t, x):
    return self.differentiable_model(x)

  def neg_loglike(self, batch_x, pred_x):
    n_samples, n_points, dammy, input_dim = batch_x.shape
    likelihood = ( (-(pred_x-batch_x)**2/self.sigma**2/2).nansum()
                   - torch.log(self.sigma**2)/2*n_samples*n_points*input_dim)
    return -likelihood

  def KL(self, x0):
    n, d = x0.shape
    S = torch.diag(self.a**2)
    return .5*((x0*x0).sum() + n*torch.trace(S) - n*torch.logdet(S))
  
  def sampling_x0(self, x0):
    n, dammy, d = x0.shape
    return (x0 + torch.sqrt(torch.stack([self.a**2]*n).reshape([n,1,d]))
            * (torch.normal(0,1, size=(x0.shape[0],1,x0.shape[2]))))

  
class SymODEN(torch.nn.Module):
  def __init__(self, input_dim, sigma, differentiable_model):
    super(SymODEN, self).__init__()
    self.differentiable_model = differentiable_model
    self.sigma = nn.Parameter(torch.tensor([sigma]))
    #self.a = nn.Parameter(torch.tensor([1e-1]))
    self.a = nn.Parameter(torch.ones(input_dim)*1e-1)
    self.M = self.permutation_tensor(input_dim)
            
  def forward(self, t, x):
    #x = x.squeeze()
    x = x.squeeze().clone().detach().requires_grad_(True)
    y = self.differentiable_model(x)
    H, dammy = y.split(1) if y.dim() == 1 else y.split(1,1)
    dH = torch.autograd.grad(H.sum(), x, create_graph=True)[0]
    field = dH @ self.M.t()
    field = field if y.dim() == 1 else field.reshape([x.shape[0],1,2])
    return field

  def neg_loglike(self, batch_x, pred_x):
    n_samples, n_points, dammy, input_dim = batch_x.shape
    likelihood = ( (-(pred_x-batch_x)**2/self.sigma**2/2).nansum()
                   - torch.log(self.sigma**2)/2*n_samples*n_points*input_dim)
    return -likelihood

  def KL(self, x0):
    n, d = x0.shape
    S = torch.diag(self.a**2)
    return .5*((x0*x0).sum() + n*torch.trace(S) - n*torch.logdet(S))

  def sampling_x0(self, x0):
    n, dammy, d = x0.shape
    return (x0 + torch.sqrt(torch.stack([self.a**2]*n).reshape([n,1,d]))
            * (torch.normal(0,1, size=(x0.shape[0],1,x0.shape[2]))))

  def permutation_tensor(self,n):
    M = torch.eye(n)
    M = torch.cat([M[n//2:], -M[:n//2]])
    return M


class D_SymODEN(torch.nn.Module):
  def __init__(self, input_dim, sigma, eta, differentiable_model):
    super(D_SymODEN, self).__init__()
    self.differentiable_model = differentiable_model
    self.sigma = nn.Parameter(torch.tensor([sigma]))
    self.eta = nn.Parameter(torch.tensor([eta]))
    self.a = nn.Parameter(torch.ones(input_dim)*1e-2)
    self.M = self.permutation_tensor(input_dim)
            
  def forward(self, t, x):
    #x = x.squeeze()
    x = x.squeeze().clone().detach().requires_grad_(True)
    y = self.differentiable_model(x)
    H, dammy = y.split(1) if y.dim() == 1 else y.split(1,1)
    dH = torch.autograd.grad(H.sum(), x, create_graph=True)[0]
    field = dH @ self.M.t()
    dH[:,0] = 0
    field = field - self.eta**2 * dH
    field = field if y.dim() == 1 else field.reshape([x.shape[0],1,2])
    return field

  def neg_loglike(self, batch_x, pred_x):
    n_samples, n_points, dammy, input_dim = batch_x.shape
    likelihood = ( (-(pred_x-batch_x)**2/self.sigma**2/2).nansum()
                   - torch.log(self.sigma**2)/2*n_samples*n_points*input_dim )
    return -likelihood# + 100 * self.eta**2

  def KL(self, x0):
    n, d = x0.shape
    S = torch.diag(self.a**2)
    return .5*((x0*x0).sum() + n*torch.trace(S) - n*torch.logdet(S))

  def sampling_x0(self, x0):
    n, dammy, d = x0.shape
    return (x0 + torch.sqrt(torch.stack([self.a**2]*n).reshape([n,1,d]))
            * (torch.normal(0,1, size=(x0.shape[0],1,x0.shape[2]))))

  def permutation_tensor(self,n):
    M = torch.eye(n)
    M = torch.cat([M[n//2:], -M[:n//2]])
    return M


class SympGPR(torch.nn.Module):
  def __init__(self, input_dim, sigma, ys, dys):
    super(SympGPR, self).__init__()
    self.alpha = nn.Parameter(torch.tensor([1.])) # kernel variance
    self.beta = nn.Parameter(torch.tensor([3.,3.])) # kernel scale
    #self.beta = nn.Parameter(torch.tensor([1.5,1.5])) # kernel scale
    self.sigma = nn.Parameter(torch.tensor([sigma])) # obs noise variance
    self.z = ys
    self.a = dys.t().reshape([dys.shape[0]*dys.shape[1],1])
    
  def forward(self, t, x):
    M, input_dim = self.z.shape
    N = x.shape[0]
    sqrt_beta = torch.sqrt(self.beta**2)
    gram = self.alpha**2 * torch.exp(-torch.cdist(x/sqrt_beta, self.z/sqrt_beta)**2/2)
    Dq = torch.cdist(x[:,0].reshape([N,1]),self.z[:,0].reshape([M,1]))**2
    Dp = torch.cdist(x[:,1].reshape([N,1]),self.z[:,1].reshape([M,1]))**2
    A = (1-Dq/self.beta[0]**2)/self.beta[0]**2*gram
    B=C = -( (torch.stack([x[:,0]]*M).t() - torch.stack([self.z[:,0]]*N))
           * (torch.stack([x[:,1]]*M).t() - torch.stack([self.z[:,1]]*N))
            )/self.beta[0]**2/self.beta[1]**2*gram
    D = (1-Dp/self.beta[1]**2)/self.beta[1]**2*gram
    Kxz = torch.cat([torch.cat([D,-B], axis=1),torch.cat([-C,A], axis=1)])
    tmp = torch.mm(Kxz,torch.linalg.solve(self.Kxx, self.a))
    drift = torch.stack(np.split(tmp,2)).squeeze().t()
    return drift

  def cov(self, x, device):
    M, input_dim = x.shape
    sqrt_beta = torch.sqrt(self.beta**2)
    gram = self.alpha**2 * torch.exp(-torch.cdist(x/sqrt_beta, x/sqrt_beta)**2/2)
    Dq = (torch.stack([x[:,0]]*M).t() - torch.stack([x[:,0]]*M))**2
    Dp = (torch.stack([x[:,1]]*M).t() - torch.stack([x[:,1]]*M))**2
    A = (1-Dq/self.beta[0]**2)/self.beta[0]**2*gram
    B=C = -( (torch.stack([x[:,0]]*M).t() - torch.stack([x[:,0]]*M))
           * (torch.stack([x[:,1]]*M).t() - torch.stack([x[:,1]]*M))
          )/self.beta[0]**2/self.beta[1]**2*gram
    D = (1-Dp/self.beta[1]**2)/self.beta[1]**2*gram
    self.Kxx = torch.cat([torch.cat([D,-B], axis=1),torch.cat([-C,A], axis=1)])
    self.Kxx += torch.eye(self.Kxx.shape[0]).to(device)*1e-2

  def neg_loglike(self, dys, device):
    y = dys.t().reshape([dys.shape[0]*dys.shape[1],1])
    Cov = self.Kxx + self.sigma**2 * torch.eye(self.Kxx.shape[0]).to(device)
    neg_loglike = -.5*torch.matmul(y.t(), torch.linalg.solve(Cov, y)) -.5*torch.slogdet(Cov)[1]
    return -neg_loglike


class D_SympGPR(torch.nn.Module):
  def __init__(self, input_dim, sigma, eta, ys, dys):
    super(D_SympGPR, self).__init__()
    self.alpha = nn.Parameter(torch.tensor([1.])) # kernel variance
    self.beta = nn.Parameter(torch.tensor([1.])) # kernel scale
    self.sigma = nn.Parameter(torch.tensor([sigma])) # obs noise variance
    self.eta = nn.Parameter(torch.tensor([eta])) # obs noise variance
    self.z = ys
    self.a = dys.t().reshape([dys.shape[0]*dys.shape[1],1])

  def forward(self, t, x):
    M, input_dim = self.z.shape
    N = x.shape[0]
    gram = self.alpha**2 * torch.exp(-torch.cdist(x, self.z)**2/2/self.beta**2)
    Dq = torch.cdist(x[:,0].reshape([N,1]),self.z[:,0].reshape([M,1]))**2
    Dp = torch.cdist(x[:,1].reshape([N,1]),self.z[:,1].reshape([M,1]))**2
    A = (1-Dq/self.beta**2)/self.beta**2*gram
    B=C = -( (torch.stack([x[:,0]]*M).t() - torch.stack([self.z[:,0]]*N))
           * (torch.stack([x[:,1]]*M).t() - torch.stack([self.z[:,1]]*N))
          )/self.beta**4*gram
    D = (1-Dp/self.beta**2)/self.beta**2*gram

    Kxz = torch.cat([torch.cat([D,-B-self.eta**2*D], axis=1),
                     torch.cat([-C-self.eta**2*D,A+2*self.eta**2*B+self.eta**4*D], axis=1)])
    tmp = torch.mm(Kxz,torch.linalg.solve(self.Kxx, self.a))
    drift = torch.stack(np.split(tmp,2)).squeeze().t()
    return drift

  def cov(self, x):
    M, input_dim = x.shape
    gram = self.alpha**2 * torch.exp(-torch.cdist(x, x)**2/2/self.beta**2)
    Dq = (torch.stack([x[:,0]]*M).t() - torch.stack([x[:,0]]*M))**2
    Dp = (torch.stack([x[:,1]]*M).t() - torch.stack([x[:,1]]*M))**2
    A = (1-Dq/self.beta**2)/self.beta**2*gram
    B = -( (torch.stack([x[:,0]]*M).t() - torch.stack([x[:,0]]*M))
           * (torch.stack([x[:,1]]*M).t() - torch.stack([x[:,1]]*M))
          )/self.beta**4*gram
    C = B
    D = (1-Dp/self.beta**2)/self.beta**2*gram
    self.Kxx = torch.cat([torch.cat([D,-B-self.eta**2*D], axis=1),
                     torch.cat([-C-self.eta**2*D,A+2*self.eta**2*B+self.eta**4*D], axis=1)])
    self.Kxx += torch.eye(self.Kxx.shape[0])*1e-2

  def neg_loglike(self, dys):
    y = dys.t().reshape([dys.shape[0]*dys.shape[1],1])
    Cov = self.Kxx + self.sigma**2 * torch.eye(self.Kxx.shape[0])
    neg_loglike = -.5*torch.matmul(y.t(), torch.linalg.solve(Cov, y)) -.5*torch.slogdet(Cov)[1]
    return -neg_loglike

class SSGP(torch.nn.Module):
  def __init__(self, input_dim, sigma, basis):
    super(SSGP, self).__init__()
    self.sigma = nn.Parameter(torch.tensor([sigma]))
    self.a = nn.Parameter(torch.ones(input_dim)*1e-1)
    self.b = nn.Parameter(1e-4 * (torch.rand(basis*2)-0.5))
    
    self.C_type = 'block2'
    if self.C_type == 'diag':
      #diagonal
      self.c = nn.Parameter(torch.ones(basis*2)*1e-1)
    elif self.C_type == 'block1':
      #block1
      self.c = nn.Parameter(torch.ones(basis*2)*1e-1)
      self.c2 = nn.Parameter(torch.ones(basis)*1e-1)
      self.ids = torch.tensor([i for i in range(basis)])
    elif self.C_type == 'block2':
      #block diagonal
      C = torch.linalg.cholesky(torch.ones(basis,basis)*1e-2+torch.eye(basis)*1e-2)
      C_line = C.reshape([(basis)**2])
      ids = torch.where(C_line!=0)[0]
      self.c = nn.Parameter(C_line[ids])
      ids = []
      for i in range(basis):
        for j in range(i+1):
          ids.append([i,j])
      ids = torch.tensor(ids)
      self.ids0 = ids[:,0]
      self.ids1 = ids[:,1]
    elif self.C_type == 'full':
      #full
      C = torch.linalg.cholesky(torch.ones(2*basis,2*basis)*1e-2+torch.eye(2*basis)*1e-1)
      C_line = C.reshape([(basis*2)**2])
      ids = torch.where(C_line!=0)[0]
      self.c = nn.Parameter(C_line[ids])
      ids = []
      for i in range(2*basis):
        for j in range(i+1):
          ids.append([i,j])
      ids = torch.tensor(ids)
      self.ids0 = ids[:,0]
      self.ids1 = ids[:,1]
    
    self.sigma_0 = nn.Parameter(torch.tensor([1e-0]))
    #self.lam = nn.Parameter(torch.ones(input_dim)*3e-0)
    self.lam = nn.Parameter(torch.ones(input_dim)*1.5e-0)
    self.M = self.permutation_tensor(input_dim)
    np.random.seed(0)
    tmp = torch.tensor(np.random.normal(0, 1, size=(int(basis/2.), input_dim)))
    self.epsilon = torch.vstack([tmp,-tmp])
    self.d = input_dim
    self.num_basis = basis

  def make_C(self):
    if self.C_type == 'diag':
      #diagonal
      C = torch.diag(self.c**2)
    elif self.C_type == 'block1':
      #diagonal2
      C = torch.diag(self.c**2)
      C[self.ids,self.ids+self.num_basis] = self.c2**2
      C[self.ids+self.num_basis,self.ids] = self.c2**2
    elif self.C_type == 'block2':
      #block diagonal
      C = torch.zeros(self.num_basis,self.num_basis)
      C[self.ids0,self.ids1] = self.c
      C = C@C.T
      #C = torch.block_diag(C,C)
    elif self.C_type == 'full':
      #full
      C = torch.zeros(2*self.num_basis,2*self.num_basis)
      C[self.ids0,self.ids1] = self.c
      C = C@C.T
    return C
    
  def sampling_epsilon_f(self):
    C = self.make_C()
    sqrt_C = sqrtm(C)
    if self.C_type == 'block2':
      sqrt_C = torch.block_diag(sqrt_C,sqrt_C)

    epsilon = torch.tensor(np.random.normal(0, 1, size=(1,sqrt_C.shape[0]))).T
    self.w = self.b + (sqrt_C @ epsilon).squeeze()
    num = 99#
    for i in range(num):
      epsilon = torch.tensor(np.random.normal(0, 1, size=(1,sqrt_C.shape[0]))).T
      self.w += self.b + (sqrt_C @ epsilon).squeeze()
    self.w = self.w/(num+1)
      
  def mean_w(self):
    self.w = self.b * 1
    
  def forward(self, t, x):
    s = self.epsilon @ torch.diag((1 / torch.sqrt(4*math.pi**2 * self.lam**2)))
    mat = 2*math.pi*(self.M@s.T).T
    x = x.squeeze()
    samples = x.shape[0]
    sim = 2*math.pi*s@x.squeeze().T # basis \times samples
    basis_s = -torch.sin(sim); basis_c = torch.cos(sim)
    W = self.w.reshape([self.num_basis,2])

    # deterministic
    mat_0=torch.stack([mat[:,0]]*samples); mat_1=torch.stack([mat[:,1]]*samples)
    tmp = torch.hstack([mat_0, mat_1]).T
    aug_mat = torch.hstack([tmp,tmp])
    aug_s = torch.vstack([basis_s]*2); aug_c = torch.vstack([basis_c]*2)
    aug_basis = torch.hstack([aug_s, aug_c])
    PHI = aug_mat * aug_basis
    W_0=torch.stack([W[:,0]]*samples); W_1=torch.stack([W[:,1]]*samples)
    tmp = torch.vstack([W_0, W_1]).T
    aug_W = torch.vstack([tmp,tmp])
    F = PHI * aug_W
    tmp = F[:,:samples] + F[:,samples:]
    f = torch.vstack([tmp[:self.num_basis,:].sum(axis=0), tmp[self.num_basis:,:].sum(axis=0)]).T    

    return f.reshape([samples,1,self.d])

  def uncertainty(self, x):
    s = self.epsilon @ torch.diag((1 / torch.sqrt(4*math.pi**2 * self.lam**2)))
    mat = 2*math.pi*(self.M@s.T).T
    x = x.squeeze()
    samples = x.shape[0]
    sim = 2*math.pi*s@x.squeeze().T # basis \times samples
    basis_s = -torch.sin(sim); basis_c = torch.cos(sim)
    #W = self.w.reshape([self.num_basis,2])

    pred_var = []
    C = self.make_C()
    if self.C_type == 'block2':
      C = torch.block_diag(C,C)

    for sam in range(samples):
      tmp_s = basis_s[:,sam]#**2
      tmp_c = basis_c[:,sam]#**2

      phi_s = []; phi_c = []
      for b in range(self.num_basis):
        phi = mat[b,:].reshape([self.d,1]) @ torch.stack([tmp_s[b],tmp_c[b]]).reshape([1,self.d])
        phi_s.append(phi[:,0])
        phi_c.append(phi[:,1])
      phi_s = torch.stack(phi_s)
      phi_c = torch.stack(phi_c)
      phi = torch.vstack([phi_s,phi_c]).T

      var = torch.sqrt(torch.diag(phi @ C @ phi.T))
      pred_var.append(var)
            
    pred_var = torch.stack(pred_var)
    return pred_var

  def neg_loglike(self, batch_x, pred_x):
    n_samples, n_points, dammy, input_dim = batch_x.shape
    likelihood = ( (-(pred_x-batch_x)**2/self.sigma**2/2).nansum()
                   - torch.log(self.sigma**2)/2*n_samples*n_points*input_dim)
    return -likelihood

  def KL_x0(self, x0):
    n, d = x0.shape
    S = torch.diag(self.a**2)
    return .5*((x0*x0).sum() + n*torch.trace(S) - n*torch.logdet(S))

  def KL_w(self):
    num = self.b.shape[0] # basis*2 (2=[sin,cos])
    C = self.make_C()
    if self.C_type == 'block2':
      C = torch.block_diag(C,C)
      
    term3 = (self.b*self.b).sum() / (self.sigma_0**2 / num * 2)
    term2 = torch.diag(C).sum() / (self.sigma_0**2 / num * 2)
    term1_1 = torch.log(1/(self.sigma_0**2 / num * 2)) * num
    term1_2 = torch.logdet(C)
    return .5*( term1_1 - term1_2 + term2 + term3)

  def sampling_x0(self, x0):
    n, dammy, d = x0.shape
    return (x0 + torch.sqrt(torch.stack([self.a**2]*n).reshape([n,1,d]))
            * (torch.normal(0,1, size=(x0.shape[0],1,x0.shape[2]))))

  def permutation_tensor(self,n):
    M = torch.eye(n)
    M = torch.cat([M[n//2:], -M[:n//2]])
    return M


class D_SSGP(torch.nn.Module):
  def __init__(self, input_dim, sigma, eta, basis):
    super(D_SSGP, self).__init__()
    self.sigma = nn.Parameter(torch.tensor([sigma]))
    self.a = nn.Parameter(torch.ones(input_dim)*1e-1)
    self.b = nn.Parameter(1e-4 * (torch.rand(basis*input_dim)-0.5))

    self.C_type = 'block2'
    if self.C_type == 'diag':
      #diagonal
      self.c = nn.Parameter(torch.ones(basis*2)*1e-1)
    elif self.C_type == 'block1':
      #block1
      self.c = nn.Parameter(torch.ones(basis*2)*1e-1)
      self.c2 = nn.Parameter(torch.ones(basis)*1e-1)
      self.ids = torch.tensor([i for i in range(basis)])
    elif self.C_type == 'block2':
      #block diagonal
      C = torch.linalg.cholesky(torch.ones(basis,basis)*1e-2+torch.eye(basis)*1e-2)
      C_line = C.reshape([(basis)**2])
      ids = torch.where(C_line!=0)[0]
      self.c = nn.Parameter(C_line[ids])
      ids = []
      for i in range(basis):
        for j in range(i+1):
          ids.append([i,j])
      ids = torch.tensor(ids)
      self.ids0 = ids[:,0]
      self.ids1 = ids[:,1]
    elif self.C_type == 'full':
      #full
      C = torch.linalg.cholesky(torch.ones(2*basis,2*basis)*1e-2+torch.eye(2*basis)*1e-1)
      C_line = C.reshape([(basis*2)**2])
      ids = torch.where(C_line!=0)[0]
      self.c = nn.Parameter(C_line[ids])
      ids = []
      for i in range(2*basis):
        for j in range(i+1):
          ids.append([i,j])
      ids = torch.tensor(ids)
      self.ids0 = ids[:,0]
      self.ids1 = ids[:,1]
    
    self.sigma_0 = nn.Parameter(torch.tensor([1e-0]))
    #self.lam = nn.Parameter(torch.ones(input_dim)*3e-0)
    self.lam = nn.Parameter(torch.ones(input_dim)*1.5e-0)
    self.eta = nn.Parameter(torch.tensor([eta]))
    self.M = self.permutation_tensor(input_dim)
    np.random.seed(0)
    tmp = torch.tensor(np.random.normal(0, 1, size=(int(basis/2.), input_dim)))
    self.epsilon = torch.vstack([tmp,-tmp])
    self.d = input_dim
    self.num_basis = basis

  def flag(self):
    self.M = 0
    
  def make_C(self):
    if self.C_type == 'diag':
      #diagonal
      C = torch.diag(self.c**2)
    elif self.C_type == 'block1':
      #diagonal2
      C = torch.diag(self.c**2)
      C[self.ids,self.ids+self.num_basis] = self.c2**2
      C[self.ids+self.num_basis,self.ids] = self.c2**2
    elif self.C_type == 'block2':
      #block diagonal
      C = torch.zeros(self.num_basis,self.num_basis)
      C[self.ids0,self.ids1] = self.c
      C = C@C.T
      #C = torch.block_diag(C,C)
    elif self.C_type == 'full':
      #full
      C = torch.zeros(2*self.num_basis,2*self.num_basis)
      C[self.ids0,self.ids1] = self.c
      C = C@C.T
    return C

  def sampling_epsilon_f(self):
    C = self.make_C()
    sqrt_C = sqrtm(C)
    if self.C_type == 'block2':
      sqrt_C = torch.block_diag(sqrt_C,sqrt_C)

    epsilon = torch.tensor(np.random.normal(0, 1, size=(1,sqrt_C.shape[0]))).T
    self.w = self.b + (sqrt_C @ epsilon).squeeze()
    num = 99
    for i in range(num):
      epsilon = torch.tensor(np.random.normal(0, 1, size=(1,sqrt_C.shape[0]))).T
      self.w += self.b + (sqrt_C @ epsilon).squeeze()
    self.w = self.w/(num+1)

  def mean_w(self):
    self.w = self.b * 1
    
  def forward(self, t, x):
    s = self.epsilon @ torch.diag((1 / torch.sqrt(4*math.pi**2 * self.lam**2)))
    #mat = ((self.M-self.eta**2)@s.T).T
    R = torch.tensor([[0,0],[0,1]])
    mat = 2*math.pi*((self.M-self.eta**2*R)@s.T).T
    x = x.squeeze()
    samples = x.shape[0]
    sim = 2*math.pi*s@x.squeeze().T # basis \times samples
    basis_s = -torch.sin(sim); basis_c = torch.cos(sim)
    W = self.w.reshape([self.num_basis,2])

    # deterministic
    mat_0=torch.stack([mat[:,0]]*samples); mat_1=torch.stack([mat[:,1]]*samples)
    tmp = torch.hstack([mat_0, mat_1]).T
    aug_mat = torch.hstack([tmp,tmp])
    aug_s = torch.vstack([basis_s]*2); aug_c = torch.vstack([basis_c]*2)
    aug_basis = torch.hstack([aug_s, aug_c])
    PHI = aug_mat * aug_basis
    W_0=torch.stack([W[:,0]]*samples); W_1=torch.stack([W[:,1]]*samples)
    tmp = torch.vstack([W_0, W_1]).T
    aug_W = torch.vstack([tmp,tmp])
    F = PHI * aug_W
    tmp = F[:,:samples] + F[:,samples:]
    f = torch.vstack([tmp[:self.num_basis,:].sum(axis=0), tmp[self.num_basis:,:].sum(axis=0)]).T
    
    return f.reshape([samples,1,self.d])

  def uncertainty(self, x):
    s = self.epsilon @ torch.diag((1 / torch.sqrt(4*math.pi**2 * self.lam**2)))
    mat = 2*math.pi*(self.M@s.T).T
    x = x.squeeze()
    samples = x.shape[0]
    sim = 2*math.pi*s@x.squeeze().T # basis \times samples
    basis_s = -torch.sin(sim); basis_c = torch.cos(sim)
    #W = self.w.reshape([self.num_basis,2])

    pred_var = []
    C = self.make_C()
    if self.C_type == 'block2':
      C = torch.block_diag(C,C)

    for sam in range(samples):
      tmp_s = basis_s[:,sam]#**2
      tmp_c = basis_c[:,sam]#**2

      phi_s = []; phi_c = []
      for b in range(self.num_basis):
        phi = mat[b,:].reshape([self.d,1]) @ torch.stack([tmp_s[b],tmp_c[b]]).reshape([1,self.d])
        phi_s.append(phi[:,0])
        phi_c.append(phi[:,1])
      phi_s = torch.stack(phi_s)
      phi_c = torch.stack(phi_c)
      phi = torch.vstack([phi_s,phi_c]).T

      var = torch.sqrt(torch.diag(phi @ C @ phi.T))
      pred_var.append(var)

    pred_var = torch.stack(pred_var)
    return pred_var

  def neg_loglike(self, batch_x, pred_x):
    n_samples, n_points, dammy, input_dim = batch_x.shape
    likelihood = ( (-(pred_x-batch_x)**2/self.sigma**2/2).nansum()
                   - torch.log(self.sigma**2)/2*n_samples*n_points*input_dim)
    return -likelihood

  def KL_x0(self, x0):
    n, d = x0.shape
    S = torch.diag(self.a**2)
    return .5*((x0*x0).sum() + n*torch.trace(S) - n*torch.logdet(S))

  def KL_w(self):
    num = self.b.shape[0] # basis*2 (2=[sin,cos])
    C = self.make_C()
    if self.C_type == 'block2':
      C = torch.block_diag(C,C)
    
    term3 = (self.b*self.b).sum() / (self.sigma_0**2 / num * 2)
    term2 = torch.diag(C).sum() / (self.sigma_0**2 / num * 2)
    term1_1 = torch.log(1/(self.sigma_0**2 / num * 2)) * num
    term1_2 = torch.logdet(C)
    return .5*( term1_1 - term1_2 + term2 + term3)

  def sampling_x0(self, x0):
    n, dammy, d = x0.shape
    return (x0 + torch.sqrt(torch.stack([self.a**2]*n).reshape([n,1,d]))
            * (torch.normal(0,1, size=(x0.shape[0],1,x0.shape[2]))))

  def permutation_tensor(self,n):
    M = torch.eye(n)
    M = torch.cat([M[n//2:], -M[:n//2]])
    return M


class RFF(torch.nn.Module):
  def __init__(self, input_dim, sigma, basis):
    super(RFF, self).__init__()
    self.sigma = nn.Parameter(torch.tensor([sigma]))
    self.a = nn.Parameter(torch.ones(input_dim)*1e-1)
    self.b = nn.Parameter(1e-4 * (torch.rand(basis*input_dim*2)-0.5))
    self.c = nn.Parameter(torch.ones(basis*input_dim*2)*1e-1)
    self.sigma_0 = nn.Parameter(torch.tensor([1e-0]))
    #self.lam = nn.Parameter(torch.ones(input_dim)*3e-0)
    self.lam = nn.Parameter(torch.ones(input_dim)*1.5e-0)
    np.random.seed(0)
    tmp = torch.tensor(np.random.normal(0, 1, size=(int(basis/2.), input_dim)))
    self.epsilon = torch.vstack([tmp,-tmp])
    self.d = input_dim
    self.num_basis = basis

  def sampling_epsilon_f(self):
    epsilon = torch.tensor(np.random.normal(0, 1, size=(1,self.c.shape[0])))
    self.w = self.b + (torch.sqrt(self.c**2) * epsilon).squeeze()
    num = 99
    for i in range(num):
      epsilon = torch.tensor(np.random.normal(0, 1, size=(1,self.c.shape[0])))
      self.w += self.b + (torch.sqrt(self.c**2) * epsilon).squeeze()
    self.w = self.w/(num+1)
      
  def mean_w(self):
    self.w = self.b * 1
    
  def forward(self, t, x):
    s = self.epsilon @ torch.diag((1 / torch.sqrt(4*math.pi**2 * self.lam**2)))
    x = x.squeeze()
    samples = x.shape[0]
    sim = 2*math.pi*s@x.squeeze().T # basis \times samples
    basis_s = torch.sin(sim); basis_c = torch.cos(sim)

    # dim 1
    W = self.w.reshape([self.num_basis,4])
    W_0 = torch.stack([W[:,0]]*samples); W_1=torch.stack([W[:,1]]*samples)
    aug_basis = torch.hstack([basis_s, basis_c])
    aug_W = torch.vstack([W_0, W_1]).T
    F = aug_basis * aug_W
    output1 = (F[:,:samples] + F[:,samples:]).sum(axis=0)

    # dim 2
    W_0 = torch.stack([W[:,2]]*samples); W_1=torch.stack([W[:,3]]*samples)
    aug_basis = torch.hstack([basis_s, basis_c])
    aug_W = torch.vstack([W_0, W_1]).T
    F = aug_basis * aug_W
    output2 = (F[:,:samples] + F[:,samples:]).sum(axis=0)

    f = torch.stack([output1,output2]).T

    return f.reshape([samples,1,self.d])

  def uncertainty(self, x):
    s = self.epsilon @ torch.diag((1 / torch.sqrt(4*math.pi**2 * self.lam**2)))
    mat = 2*math.pi*(self.M@s.T).T
    x = x.squeeze()
    samples = x.shape[0]
    sim = 2*math.pi*s@x.squeeze().T # basis \times samples
    basis_s = -torch.sin(sim); basis_c = torch.cos(sim)
    #W = self.w.reshape([self.num_basis,2])

    pred_var = []
    C = self.c.reshape([self.num_basis,2])
    #C = torch.stack([self.c[:self.num_basis],self.c[self.num_basis:]]).T

    for sam in range(samples):
      tmp_s = basis_s[:,sam]#**2
      tmp_c = basis_c[:,sam]#**2
      #pdb.set_trace()
      var = 0
      for b in range(self.num_basis):
        phi = mat[b,:].reshape([self.d,1]) @ torch.stack([tmp_s[b],tmp_c[b]]).reshape([1,self.d])
        var += torch.diag(phi @ torch.diag(C[b,:]**2) @ phi.T)
        #if b == 0:
        #  var = torch.diag((phi @ phi.T)*1e-2)
        #  print(sam,var)
        #  pdb.set_trace()
        #else:
        #  var += torch.diag((phi @ phi.T)*1e-2)
        #print(phi @ torch.diag(C[b,:]**2) @ phi.T)
      pred_var.append(var)

    pred_var = torch.sqrt(torch.stack(pred_var).sum(axis=1)/2.)
    #pred_var = torch.sqrt(torch.stack(pred_var)[:,1])
    return pred_var

  def neg_loglike(self, batch_x, pred_x):
    n_samples, n_points, dammy, input_dim = batch_x.shape
    likelihood = ( (-(pred_x-batch_x)**2/self.sigma**2/2).nansum()
                   - torch.log(self.sigma**2)/2*n_samples*n_points*input_dim)
    return -likelihood

  def KL_x0(self, x0):
    n, d = x0.shape
    S = torch.diag(self.a**2)
    return .5*((x0*x0).sum() + n*torch.trace(S) - n*torch.logdet(S))

  def KL_w(self):
    num = self.b.shape[0] # basis*2 (2=[sin,cos])
    term3 = (self.b*self.b).sum() / (self.sigma_0**2 / num * 2)
    term2 = (self.c**2).sum() / (self.sigma_0**2 / num * 2)
    #term1_1 = torch.logdet(torch.eye(2) * (self.sigma_0**2 / num * 2)) * num/2.
    term1_1 = torch.log(1/(self.sigma_0**2 / num * 2)) * num
    #C = (self.c**2).reshape([self.num_basis,2])
    #Z = torch.zeros(self.num_basis)
    #term1_2 = torch.logdet((torch.stack([C[:,0],Z,Z,C[:,1]]).T).reshape([self.num_basis,self.d,self.d])).sum()
    term1_2 = torch.log(1/self.c**2).sum()
    return .5*( term1_1 - term1_2 + term2 + term3)

  def sampling_x0(self, x0):
    n, dammy, d = x0.shape
    return (x0 + torch.sqrt(torch.stack([self.a**2]*n).reshape([n,1,d]))
            * (torch.normal(0,1, size=(x0.shape[0],1,x0.shape[2]))))

  def permutation_tensor(self,n):
    M = torch.eye(n)
    M = torch.cat([M[n//2:], -M[:n//2]])
    return M
